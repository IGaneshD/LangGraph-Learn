{
 "cells": [
  {
   "cell_type": "code",
   "id": "b54a9d8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:04:17.488151Z",
     "start_time": "2025-08-02T10:04:17.476547Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "# load the environment variables\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "00d2e7be",
   "metadata": {},
   "source": [
    "**NOTE** Python uses .rst format for sphinix documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46df8e80",
   "metadata": {},
   "source": [
    "### Testing OpenAI client "
   ]
  },
  {
   "cell_type": "code",
   "id": "eb6d86a1",
   "metadata": {},
   "source": [
    "# importing openai client from langchain_openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# print(OpenAI.__doc__)\n",
    "\n",
    "# need to pass \n",
    "\n",
    "# base_url --> OPENAI_API_BASE\n",
    "# api_key  --> OPENAI_API_KEY\n",
    "\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(base_url=os.getenv(\"OPENAI_API_BASE\"), api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "llm.invoke(\"Who are you?\").__dict__"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3cdef843",
   "metadata": {},
   "source": [
    "# Human Message Object\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "SystemPrompt = SystemMessage(\"You are an Expert Python Developer. \" \\\n",
    "\"Your Task is to write UnitTestCases for given Python Code using **PyTest** Framework. Do not add Extra Commentry\")\n",
    "\n",
    "code_input = \"\"\"\n",
    "\n",
    "```python\n",
    "def add(x,y):\n",
    "    return x + y\n",
    "```\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [SystemPrompt, code_input]\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "response.__dict__\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d31b5424",
   "metadata": {},
   "source": [
    "print(response.content)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8681ad85",
   "metadata": {},
   "source": [
    "response.usage_metadata"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing Azure Client",
   "id": "d6f3851d708e800c"
  },
  {
   "cell_type": "code",
   "id": "bee9730d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:54:48.138580Z",
     "start_time": "2025-08-02T10:54:47.706943Z"
    }
   },
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "azure_client = AzureChatOpenAI(api_version=os.getenv(\"AZURE_OPENAI_VERSION\"), azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"))"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:36:01.496986Z",
     "start_time": "2025-08-02T10:35:59.590438Z"
    }
   },
   "cell_type": "code",
   "source": "response = azure_client.invoke(\"Generate a random json for user schema and only use 5-6 fields\")",
   "id": "554a1fb417f21a11",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "response",
   "id": "bde227eb3b90e0b5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Structured Output",
   "id": "c163b0cba62c89fd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:59:06.717764Z",
     "start_time": "2025-08-02T10:59:06.710522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel, Field\n",
    "class ResponseFormatter(BaseModel):\n",
    "    \"\"\"Always use this tool to structure your response to the user.\"\"\"\n",
    "    answer: str = Field(description=\"The answer to the user's question\")\n",
    "    followup_question: str = Field(description=\"A followup question the user could ask\")\n",
    "\n",
    "\n",
    "\n",
    "structured_model = azure_client.bind_tools([ResponseFormatter])"
   ],
   "id": "96ad058f83664776",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:59:11.113049Z",
     "start_time": "2025-08-02T10:59:09.491608Z"
    }
   },
   "cell_type": "code",
   "source": "structured_output = structured_model.invoke(\"Who is the President of India?\")",
   "id": "af8205f1fa2bd38b",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T10:59:23.955079Z",
     "start_time": "2025-08-02T10:59:23.943545Z"
    }
   },
   "cell_type": "code",
   "source": "structured_output",
   "id": "eb9663fcf5b7733d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_iIv4rvG70A4mW2iZEphbf5QE', 'function': {'arguments': '{\"answer\":\"As of October 2023, the President of India is Droupadi Murmu. She took office on July 25, 2022, becoming the first tribal woman to hold the position.\",\"followup_question\":\"What are the main responsibilities of the President of India?\"}', 'name': 'ResponseFormatter'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 79, 'total_tokens': 148, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_efad92c60b', 'id': 'chatcmpl-C04FmXDqJgmN3VOGa9IzauxOoLZmO', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'tool_calls', 'logprobs': None, 'content_filter_results': {}}, id='run--6559abcc-b806-4356-9d87-0a5fadf5e48e-0', tool_calls=[{'name': 'ResponseFormatter', 'args': {'answer': 'As of October 2023, the President of India is Droupadi Murmu. She took office on July 25, 2022, becoming the first tribal woman to hold the position.', 'followup_question': 'What are the main responsibilities of the President of India?'}, 'id': 'call_iIv4rvG70A4mW2iZEphbf5QE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 79, 'output_tokens': 69, 'total_tokens': 148, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T11:01:37.298104Z",
     "start_time": "2025-08-02T11:01:37.291662Z"
    }
   },
   "cell_type": "code",
   "source": "structured_output.__dict__",
   "id": "10e7ed886eed4c2f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'call_iIv4rvG70A4mW2iZEphbf5QE',\n",
       "    'function': {'arguments': '{\"answer\":\"As of October 2023, the President of India is Droupadi Murmu. She took office on July 25, 2022, becoming the first tribal woman to hold the position.\",\"followup_question\":\"What are the main responsibilities of the President of India?\"}',\n",
       "     'name': 'ResponseFormatter'},\n",
       "    'type': 'function'}],\n",
       "  'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 69,\n",
       "   'prompt_tokens': 79,\n",
       "   'total_tokens': 148,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-mini-2024-07-18',\n",
       "  'system_fingerprint': 'fp_efad92c60b',\n",
       "  'id': 'chatcmpl-C04FmXDqJgmN3VOGa9IzauxOoLZmO',\n",
       "  'service_tier': None,\n",
       "  'prompt_filter_results': [{'prompt_index': 0,\n",
       "    'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
       "     'jailbreak': {'filtered': False, 'detected': False},\n",
       "     'self_harm': {'filtered': False, 'severity': 'safe'},\n",
       "     'sexual': {'filtered': False, 'severity': 'safe'},\n",
       "     'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'logprobs': None,\n",
       "  'content_filter_results': {}},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run--6559abcc-b806-4356-9d87-0a5fadf5e48e-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'ResponseFormatter',\n",
       "   'args': {'answer': 'As of October 2023, the President of India is Droupadi Murmu. She took office on July 25, 2022, becoming the first tribal woman to hold the position.',\n",
       "    'followup_question': 'What are the main responsibilities of the President of India?'},\n",
       "   'id': 'call_iIv4rvG70A4mW2iZEphbf5QE',\n",
       "   'type': 'tool_call'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 79,\n",
       "  'output_tokens': 69,\n",
       "  'total_tokens': 148,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f775733ea71014f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Testing OpenAI Client by changing the Base URL to Azure",
   "id": "ac85f265a0cc0fe5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:51:48.623636Z",
     "start_time": "2025-08-02T12:51:48.037844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "llm = OpenAI(base_url=os.getenv(\"AZURE_OPENAI_ENDPOINT\"), api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"), model=\"gpt-4o-mini\")"
   ],
   "id": "cdaadf7a494e8190",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-02T12:51:59.711452Z",
     "start_time": "2025-08-02T12:51:58.148586Z"
    }
   },
   "cell_type": "code",
   "source": "llm.invoke(\"Hi\")",
   "id": "2c9d8cb360f5ffba",
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotFoundError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[37]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[43mllm\u001B[49m\u001B[43m.\u001B[49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mHi\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:389\u001B[39m, in \u001B[36mBaseLLM.invoke\u001B[39m\u001B[34m(self, input, config, stop, **kwargs)\u001B[39m\n\u001B[32m    378\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    379\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34minvoke\u001B[39m(\n\u001B[32m    380\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    385\u001B[39m     **kwargs: Any,\n\u001B[32m    386\u001B[39m ) -> \u001B[38;5;28mstr\u001B[39m:\n\u001B[32m    387\u001B[39m     config = ensure_config(config)\n\u001B[32m    388\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[32m--> \u001B[39m\u001B[32m389\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    390\u001B[39m \u001B[43m            \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_convert_input\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    391\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    392\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcallbacks\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    393\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtags\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtags\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    394\u001B[39m \u001B[43m            \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmetadata\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    395\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_name\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    396\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrun_id\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpop\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mrun_id\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    397\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    398\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    399\u001B[39m         .generations[\u001B[32m0\u001B[39m][\u001B[32m0\u001B[39m]\n\u001B[32m    400\u001B[39m         .text\n\u001B[32m    401\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:766\u001B[39m, in \u001B[36mBaseLLM.generate_prompt\u001B[39m\u001B[34m(self, prompts, stop, callbacks, **kwargs)\u001B[39m\n\u001B[32m    757\u001B[39m \u001B[38;5;129m@override\u001B[39m\n\u001B[32m    758\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mgenerate_prompt\u001B[39m(\n\u001B[32m    759\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    763\u001B[39m     **kwargs: Any,\n\u001B[32m    764\u001B[39m ) -> LLMResult:\n\u001B[32m    765\u001B[39m     prompt_strings = [p.to_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[32m--> \u001B[39m\u001B[32m766\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:971\u001B[39m, in \u001B[36mBaseLLM.generate\u001B[39m\u001B[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[39m\n\u001B[32m    956\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m.cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[32m    957\u001B[39m     run_managers = [\n\u001B[32m    958\u001B[39m         callback_manager.on_llm_start(\n\u001B[32m    959\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m    969\u001B[39m         )\n\u001B[32m    970\u001B[39m     ]\n\u001B[32m--> \u001B[39m\u001B[32m971\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    972\u001B[39m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    973\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    974\u001B[39m \u001B[43m        \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    975\u001B[39m \u001B[43m        \u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    976\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    977\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    978\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) > \u001B[32m0\u001B[39m:\n\u001B[32m    979\u001B[39m     run_managers = [\n\u001B[32m    980\u001B[39m         callback_managers[idx].on_llm_start(\n\u001B[32m    981\u001B[39m             \u001B[38;5;28mself\u001B[39m._serialized,\n\u001B[32m   (...)\u001B[39m\u001B[32m    988\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m missing_prompt_idxs\n\u001B[32m    989\u001B[39m     ]\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:792\u001B[39m, in \u001B[36mBaseLLM._generate_helper\u001B[39m\u001B[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[39m\n\u001B[32m    781\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_generate_helper\u001B[39m(\n\u001B[32m    782\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    783\u001B[39m     prompts: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mstr\u001B[39m],\n\u001B[32m   (...)\u001B[39m\u001B[32m    788\u001B[39m     **kwargs: Any,\n\u001B[32m    789\u001B[39m ) -> LLMResult:\n\u001B[32m    790\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m    791\u001B[39m         output = (\n\u001B[32m--> \u001B[39m\u001B[32m792\u001B[39m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    793\u001B[39m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    794\u001B[39m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    795\u001B[39m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[32m    796\u001B[39m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[43m=\u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    797\u001B[39m \u001B[43m                \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    798\u001B[39m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    799\u001B[39m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[32m    800\u001B[39m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m._generate(prompts, stop=stop)\n\u001B[32m    801\u001B[39m         )\n\u001B[32m    802\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    803\u001B[39m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\langchain_openai\\llms\\base.py:332\u001B[39m, in \u001B[36mBaseOpenAI._generate\u001B[39m\u001B[34m(self, prompts, stop, run_manager, **kwargs)\u001B[39m\n\u001B[32m    316\u001B[39m     choices.append(\n\u001B[32m    317\u001B[39m         {\n\u001B[32m    318\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mtext\u001B[39m\u001B[33m\"\u001B[39m: generation.text,\n\u001B[32m   (...)\u001B[39m\u001B[32m    329\u001B[39m         }\n\u001B[32m    330\u001B[39m     )\n\u001B[32m    331\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m332\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m=\u001B[49m\u001B[43m_prompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    333\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    334\u001B[39m         \u001B[38;5;66;03m# V1 client returns the response in an PyDantic object instead of\u001B[39;00m\n\u001B[32m    335\u001B[39m         \u001B[38;5;66;03m# dict. For the transition period, we deep convert it to dict.\u001B[39;00m\n\u001B[32m    336\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[39m, in \u001B[36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    285\u001B[39m             msg = \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[32m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    286\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[32m--> \u001B[39m\u001B[32m287\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\openai\\resources\\completions.py:541\u001B[39m, in \u001B[36mCompletions.create\u001B[39m\u001B[34m(self, model, prompt, best_of, echo, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, seed, stop, stream, stream_options, suffix, temperature, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    512\u001B[39m \u001B[38;5;129m@required_args\u001B[39m([\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m], [\u001B[33m\"\u001B[39m\u001B[33mmodel\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mprompt\u001B[39m\u001B[33m\"\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m    513\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcreate\u001B[39m(\n\u001B[32m    514\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    539\u001B[39m     timeout: \u001B[38;5;28mfloat\u001B[39m | httpx.Timeout | \u001B[38;5;28;01mNone\u001B[39;00m | NotGiven = NOT_GIVEN,\n\u001B[32m    540\u001B[39m ) -> Completion | Stream[Completion]:\n\u001B[32m--> \u001B[39m\u001B[32m541\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    542\u001B[39m \u001B[43m        \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/completions\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    543\u001B[39m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmodel\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    546\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mprompt\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    547\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mbest_of\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mbest_of\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    548\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mecho\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mecho\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    549\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mfrequency_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    550\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogit_bias\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    551\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mlogprobs\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    552\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmax_tokens\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    553\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mn\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    554\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mpresence_penalty\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    555\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mseed\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    556\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstop\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    557\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    558\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mstream_options\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    559\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msuffix\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43msuffix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    560\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtemperature\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    561\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mtop_p\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    562\u001B[39m \u001B[43m                \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43muser\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    563\u001B[39m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    564\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[32m    565\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    568\u001B[39m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    569\u001B[39m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\n\u001B[32m    570\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    571\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    572\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    573\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mCompletion\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    574\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\openai\\_base_client.py:1256\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1242\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1243\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1244\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1251\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1252\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1253\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1254\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1255\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1256\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mE:\\Ganesh Deshmukh\\Artificial Intelligence\\LangChain-AI\\LangGraph\\Learn\\LangGraph-Learn\\lc-academy-env\\Lib\\site-packages\\openai\\_base_client.py:1044\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1041\u001B[39m             err.response.read()\n\u001B[32m   1043\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1044\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1046\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1048\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mNotFoundError\u001B[39m: Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b553571788da4401"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lc-academy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
